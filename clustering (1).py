# -*- coding: utf-8 -*-
"""Clustering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FU0AzZYBFbJP6C1xFUJr_GOO7b53uhqM
"""

# data manipulation
import pandas as pd

# data viz
import matplotlib.pyplot as plt

# advance viz
import seaborn as sns

# scikit learn | machine learning
# import sklearn ti import dataset
from sklearn.datasets import load_wine

"""# DATA LOADING"""

wine = load_wine()
wine

df = pd.DataFrame(data=wine.data, columns=wine.feature_names)
df

df.head()

df.tail()

df.info()

df.describe()

df.isnull().sum()

print(df.duplicated().sum())

# MACINE LEARNING
#Supervised | jis mein target column ho | wine(target) - iris(species) - tianic(survived)
# 1. classification | descrete data | wine - iris - titanic
# 2. regression | continous
#Unsupervised
#Reinforcement

# EDA
# Dataset fall into classification or regression
# target | EDA | Problem
# Feature | EDA | problem | feature engineering | feature selection | feature scaling

# SMOTE | upscale method
# imblance dealing upscalr or downscale
# upscale (resample) | to increase data
# downscale | to decrease data set

df["magnesium"].describe()

#histogram plot
df.hist("magnesium", bins = 10)

print(f"Skewness: {df['magnesium'].skew()}")
print(f"Kurtosis: {df['magnesium'].kurt()}")

df.corr()

# Correlation heatmap
plt.figure(figsize= (10,10))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='Pastel1')
plt.title('Correlation Matrix of Wine Dataset')
plt.show()

"""### KMean"""

import numpy as np

X = wine.data
y = wine.target

# Calculate the minimum and maximum y-values
y_min, y_max = np.min(X[:, 1]), np.max(X[:, 1])

# Scale and shift the y-values to fit within the desired range
X[:, 1] = (X[:, 1] - y_min) / (y_max - y_min)  # Normalize to 0-1 range
X[:, 1] = X[:, 1] * 2 + 1  # Scale and shift to 1-3 range

# Visualize the dataset
plt.scatter(X[:, 0], X[:, 1], s=5)
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('An unlabeled dataset')
plt.ylim(1, 3)  # Set the y-axis limits
plt.show()

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5, random_state=0, n_init=10).fit(X[:, :2]) # Train KMeans on only the first two features
y_kmeans = kmeans.predict(X[:, :2]) # Predict on the first two features

# Visualize the clusters
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

# Plot the centroids
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering')
plt.show()

kmeans.cluster_centers_

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

# Predict cluster labels for each point in the meshgrid
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

# Plot the decision boundaries
plt.figure(1)
plt.clf()
plt.imshow(Z, interpolation='nearest',
           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
           cmap=plt.cm.Paired,
           aspect='auto', origin='lower')

plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)
plt.title('K-Means clustering with boundary')
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.xticks(())
plt.yticks(())
plt.show()

inertia_values = []

# Test different values of k
for k in range(2, 9):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertia_values.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(range(2, 9), inertia_values)
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Curve')
plt.show()

from sklearn.metrics import silhouette_score
silhouette_score(X, kmeans.labels_)

import matplotlib.pyplot as plt

# Example data â€” replace with your actual silhouette scores
k_values = [2, 3, 4, 5, 6, 7, 8]
silhouette_scores = [0.6, 0.625, 0.675, 0.7, 0.55, 0.575, 0.55]

# Plotting the silhouette scores
plt.figure(figsize=(8, 6))
plt.plot(k_values, silhouette_scores, marker='o', linestyle='-', color='b')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Selecting the Number of Clusters using Silhouette Score')
plt.grid(True)
plt.show()

"""###Heicharcy"""

df.head()

from sklearn.preprocessing import StandardScaler

features = df[['alcohol', 'magnesium']]

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

from scipy.cluster.hierarchy import dendrogram, linkage

# Calculate the distance matrix using Euclidean distance
linked = linkage(df[['alcohol']], 'single')

# Plot the dendrogram
plt.figure(figsize=(10, 7))
dendrogram(linked,
            orientation='top',
            distance_sort='descending',
            show_leaf_counts=True)
plt.title('Dendrogram (Single Linkage)')
plt.xlabel('alcohol')
plt.ylabel('magnesium')
plt.show()

"""### DBScan"""

from sklearn.cluster import DBSCAN
import numpy as np
import matplotlib.pyplot as plt


X = np.array([[1, 2], [2, 2], [2, 3],[8, 7], [8, 8], [25, 80]])

plt.scatter(X[:, 0], X[:, 1])
plt.show()

db = DBSCAN(eps=5, min_samples=3)

db.fit(X)
db.labels_

import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.cluster import DBSCAN
import numpy as np

# Create a concentric circle dataset
X, _ = make_circles(n_samples=500, factor=.5, noise=.03, random_state=4)

# Apply DBSCAN to the dataset
dbscan = DBSCAN(eps=0.1, min_samples=5)
clusters = dbscan.fit_predict(X)

# Plotting
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', marker='o')
plt.title("DBSCAN Clustering of Concentric Circles")
plt.xlabel("Feature 0")
plt.ylabel("Feature 1")
plt.show()

